# Rust Core for Shogi

[![codecov](https://codecov.io/gh/SH11235/shogi/branch/main/graph/badge.svg?flag=rust-core)](https://codecov.io/gh/SH11235/shogi)

This package contains the WebAssembly (WASM) implementation for advanced Shogi features including WebRTC communication, mate search, and opening book functionality.

## Features

- ğŸŒ WebRTC peer-to-peer communication for online play
- ğŸ” Mate search algorithm implementation
- ğŸ“š Opening book with binary format support
- ğŸ¯ High-performance position hashing and move encoding
- ğŸ¤– USI protocol engine with multiple search/evaluation modes
- ğŸ§  NNUE evaluation function support
- âš¡ Enhanced search with advanced pruning techniques
- ğŸ“Š NNUE training tools for machine learning

## Prerequisites

- Rust toolchain (install from https://rustup.rs/)
- wasm-pack (`cargo install wasm-pack`)
- cargo-tarpaulin (optional, for coverage reports): `cargo install cargo-tarpaulin`

## Project Structure

```
crates/
â”œâ”€â”€ engine-core/             # Core engine implementation
â”‚   â”œâ”€â”€ search/             # Search algorithms (basic & enhanced)
â”‚   â”œâ”€â”€ evaluation/         # Evaluation functions (material & NNUE)
â”‚   â””â”€â”€ time_management/    # Time control
â”œâ”€â”€ engine-usi/              # USI protocol command-line interface
â””â”€â”€ webrtc-p2p/             # WebRTC communication

src/                         # Legacy WASM modules
â”œâ”€â”€ lib.rs                   # Main library entry point
â”œâ”€â”€ simple_webrtc.rs        # WebRTC implementation
â”œâ”€â”€ mate_search.rs          # Mate search algorithm
â”œâ”€â”€ opening_book/           # Opening book module
â”‚   â”œâ”€â”€ mod.rs             # Module exports
â”‚   â”œâ”€â”€ binary_converter.rs # Binary format conversion
â”‚   â”œâ”€â”€ data_structures.rs  # Core data types
â”‚   â”œâ”€â”€ move_encoder.rs     # Move encoding/decoding
â”‚   â”œâ”€â”€ position_filter.rs  # Position filtering logic
â”‚   â”œâ”€â”€ position_hasher.rs  # Position hashing
â”‚   â””â”€â”€ sfen_parser.rs      # SFEN format parsing
â””â”€â”€ opening_book_reader.rs  # Opening book reader interface
```

## Documentation

- [Engine Types Guide](docs/engine-types-guide.md) - ã‚¨ãƒ³ã‚¸ãƒ³ã‚¿ã‚¤ãƒ—ã®é¸æŠã‚¬ã‚¤ãƒ‰ï¼ˆæ¨å¥¨: EnhancedNnueï¼‰
- [Performance Documentation](docs/performance/) - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã€æ€§èƒ½åˆ†æ
- [Development Guide](docs/development/) - TDDé–‹ç™ºã‚¬ã‚¤ãƒ‰ã€ãƒ†ã‚¹ãƒˆæˆ¦ç•¥
- [Implementation Docs](docs/implementation/) - å®Ÿè£…è©³ç´°
- [Reference](docs/reference/) - ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä»•æ§˜ãªã©

## USI Engine Usage

### Quick Start
```bash
# Build and run the USI engine
cargo build --release --bin engine-usi
./target/release/engine-usi

# Set to strongest mode (EnhancedNnue)
setoption name EngineType value EnhancedNnue

# Basic commands
usi
isready
position startpos
go movetime 1000
quit
```

### Performance Build & Features

- æ¨å¥¨ãƒ“ãƒ«ãƒ‰ï¼ˆæœ€é©åŒ–ï¼‰
  - `RUSTFLAGS="-C target-cpu=native" cargo run -p engine-usi --release`
- ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ï¼ˆengine-usi ã‹ã‚‰ engine-core ã¸ä¼æ’­ï¼‰
  - æ—¢å®šON: `nnue-diff`ï¼ˆSINGLE å·®åˆ†NNUEï¼‰
  - ä»»æ„ON:
    - `fast-fma`: FMAã§å‡ºåŠ›åŠ ç®—ã‚’é«˜é€ŸåŒ–ï¼ˆä¸¸ã‚å¾®å·®ã‚’è¨±å®¹ã§ãã‚‹å ´åˆï¼‰
    - `diff-agg-hash`: å·®åˆ†é›†è¨ˆã‚’HashMapå®Ÿè£…ã§A/Bï¼ˆå¤§Nå‘ã‘ï¼‰
    - `nnue-telemetry`: è»½é‡ãƒ†ãƒ¬ãƒ¡ãƒˆãƒªï¼ˆæ¢ç´¢ä¸­ã®çµŒè·¯å‰²åˆãªã©ï¼‰
    - `tt-metrics`, `ybwc`, `nightly`: å¿…è¦ã«å¿œã˜ã¦

ä¾‹: å·®åˆ†NNUE + FMA æœ‰åŠ¹
```bash
RUSTFLAGS="-C target-cpu=native" \
cargo run -p engine-usi --release --features fast-fma
```

æ³¨: fp32 è¡ŒåŠ ç®—ç”¨ SIMD ã¯ Dispatcher ã«çµ±åˆæ¸ˆã¿ã§å¸¸æ™‚ONï¼ˆå®Ÿè¡Œæ™‚ CPU æ¤œå‡º: AVX/FMA/SSE2/NEON/Scalarï¼‰ã€‚`simd` ãƒ•ã‚£ãƒ¼ãƒãƒ£ã¯ä¸è¦ã§ã™ã€‚

èµ·å‹•æ™‚ã« `info string core_features=engine-core:...` ã‚’å‡ºåŠ›ã—ã¾ã™ï¼ˆå†ç¾æ€§ãƒ»ãƒ­ã‚°ç”¨é€”ï¼‰ã€‚

### Engine Types
- **EnhancedNnue** (æ¨å¥¨): æœ€å¼· - é«˜åº¦ãªæ¢ç´¢ + NNUEè©•ä¾¡
- **Nnue**: é«˜é€Ÿåˆ†æç”¨
- **Enhanced**: çœãƒ¡ãƒ¢ãƒªç’°å¢ƒç”¨
- **Material**: ãƒ‡ãƒãƒƒã‚°ç”¨

### Engine Options

| Option | Type | Default | Range | Description |
|--------|------|---------|-------|-------------|
| USI_Hash | Spin | 16 | 1-1024 | Hash table size in MB |
| Threads | Spin | 1 | 1-256 | Number of search threads |
| USI_Ponder | Check | true | true/false | Enable pondering (thinking on opponent's time) |
| EngineType | Combo | Material | Material/Nnue/Enhanced/EnhancedNnue | Engine evaluation and search type |
| ByoyomiPeriods | Spin | 1 | 1-10 or 'default' | Number of byoyomi periods (USI_ByoyomiPeriods alias also supported) |

#### ByoyomiPeriods Option

Controls the number of byoyomi periods when using byoyomi time control:

```bash
# Set default number of periods (used when not specified in go command)
setoption name ByoyomiPeriods value 3
# or using the alias
setoption name USI_ByoyomiPeriods value 3

# Reset to default (1 period)
setoption name ByoyomiPeriods value default

# Override in go command
go byoyomi 30000 periods 5  # 5 periods of 30 seconds each
```

## Building

### From project root
```bash
npm run build:wasm      # Production build (optimized)
npm run build:wasm:dev  # Development build (faster)
```


## Important Notes

âš ï¸ **WASM files must be built before running the web application!**

The build process:
1. Compiles Rust code to WebAssembly
2. Generates JavaScript bindings and TypeScript definitions
3. Copies the generated files to `packages/web/src/wasm/`

The generated files in `packages/web/src/wasm/` are:
- Excluded from git (in .gitignore)
- Required for the web application to run
- Must be regenerated when Rust code changes

## Development Workflow

1. Make changes to Rust code
2. Run quality checks: `cargo fmt`, `cargo clippy`, `cargo test`

## Testing

```bash
# Run standard Rust tests
cargo test

# Run WASM tests in browser (requires Chrome)
wasm-pack test --chrome --headless

# Generate code coverage report (requires cargo-tarpaulin)
cargo tarpaulin --out html --lib  # Generates tarpaulin-report.html
cargo tarpaulin --out Xml  # Generates cobertura.xml for CI

# Benchmark tests (ignored by default due to execution time)
cargo test -- --ignored              # Run only ignored tests (benchmarks)
cargo test -- --include-ignored      # Run all tests including benchmarks
cargo test test_benchmark -- --ignored  # Run specific benchmark test
```

## Code Quality

### Required Checks (run automatically on pre-commit)
```bash
cargo fmt                    # Format code
cargo clippy -- -D warnings  # Lint with warnings as errors
cargo check                  # Fast type checking
```

### Additional Tools
```bash
cargo audit      # Security vulnerability scan
cargo outdated   # Check for outdated dependencies
cargo machete    # Find unused dependencies (requires installation)
```

## API Documentation

### WebRTC Module
Provides simple WebRTC functionality for peer-to-peer connections:
- Connection establishment
- Message passing
- Error handling

### Mate Search Module
Implements efficient mate search algorithms:
- Depth-limited search
- Move ordering optimization
- Performance-oriented design

### Opening Book Module
Handles opening book data in binary format:
- **Binary Format**: Compact storage of positions and moves
- **Position Hashing**: Fast lookup using FNV-1a algorithm
- **Move Encoding**: Efficient 16-bit move representation
- **SFEN Support**: Parse and convert SFEN notation
- **Database**: Currently supports 100,000+ opening positions

### NNUE Training Tools
Machine learning tools for NNUE evaluation function:
- **train_wdl_baseline**: Lightweight WDL (Win/Draw/Loss) trainer for pipeline validation
- **train_nnue**: Full NNUE trainer with HalfKP features and row-sparse updates
  - Performance metrics: loader_ratio and examples/sec monitoring
  - Cache support for faster data loading
  - Minimal training dashboard: per-epoch metrics, phase metrics, calibration (CP-binned ECE)

See tools README for usage, options, and outputs:
- crates/tools/README.md (Minimal Training Dashboard: baseline and NNUE)

#### æ‰‹å‹•ãƒ™ãƒ³ãƒï¼ˆGitHub Actionsï¼‰: NNUE Stream Loader Bench
- ç›®çš„: stream-cache ãƒ­ãƒ¼ãƒ€ã¨ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã®åŠ¹æœæ¤œè¨¼ï¼ˆsps / loader_ratio ã‚’æ¯”è¼ƒï¼‰ã€‚
- å®Ÿè¡Œ: GitHub Actions â†’ ã€ŒNNUE Stream Loader Bench (manual)ã€â†’ Run workflowã€‚
- ä»•æ§˜: å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚’åˆæˆã—ã€prefetch=0ï¼ˆåŒæœŸï¼‰/8ï¼ˆéåŒæœŸï¼‰ã§ 1 epoch å®Ÿè¡Œã€‚ã‚¸ãƒ§ãƒ–ã‚µãƒãƒªã« sps ã¨ loader_ratio ã‚’å‡ºåŠ›ã€‚
- å‚™è€ƒ: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ gzip ã‚’ä½¿ç”¨ï¼ˆzstd æ©Ÿèƒ½ã¯ä¸è¦ï¼‰ã€‚ã—ãã„å€¤ã«ã‚ˆã‚‹è‡ªå‹•å¤±æ•—ã¯æœªè¨­å®šï¼ˆå¿…è¦ãªã‚‰è¿½åŠ ï¼‰ã€‚
 - å…¥åŠ›ã¯ JSONL / Cache ã‚’è‡ªå‹•åˆ¤å®šï¼ˆCache ã¯ NNFC ãƒã‚¸ãƒƒã‚¯ãƒ˜ãƒƒãƒ€ã§æ¤œå‡ºï¼‰

ä¾‹: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å­¦ç¿’ï¼ˆäº‹å‰ãƒ­ãƒ¼ãƒ‰ãªã—ï¼‰
```bash
cargo run -p tools --bin train_nnue -- \
  -i runs/data.cache.gz -e 1 -b 16384 \
  --stream-cache --prefetch-batches 8 --throughput-interval 2.0
# ãƒ­ã‚°: [throughput] mode=stream ... sps=... loader_ratio=...%
```

è£œè¶³:
- `loader_ratio` ã¯ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ™‚ã®ã€Œãƒ­ãƒ¼ãƒ€å¾…ã¡ï¼ˆI/O/è§£å‡/å—ä¿¡å¾…æ©Ÿï¼‰ã€ãŒå ã‚ã‚‹æ¯”ç‡ã§ã™ã€‚
- äº‹å‰ãƒ¡ãƒ¢ãƒªãƒ­ãƒ¼ãƒ‰ï¼ˆinâ€‘memoryï¼‰ã§ã¯ `mode=inmem` ã§å‡ºåŠ›ã•ã‚Œã€`loader_ratio` ã¯æ¦‚ã­ 0% ã«ãªã‚Šã¾ã™ã€‚
- **build_feature_cache**: Pre-extract HalfKP features to binary cache format
  - Eliminates SFEN parsing and feature extraction overhead
  - Variable-length record format with metadata preservation
- **JSONL Support**: Direct training from annotated game data
- **Feature extraction**: HalfKP feature generation from positions

#### Training Data Generation (Streaming SFEN)

The generator now streams SFEN input to keep peak memory nearly constant, even for very large corpora. The manifest format and orchestrator integration remain unchanged.

- Input: plain text lines containing `sfen ...` (optionally with trailing `moves`), supports `-` (stdin) and compressed files (`.gz`, `.zst` when built with `zstd` feature).
- Output: JSONL or text, optional part-splitting and compression, plus v2 manifest next to outputs.
- Resume: If the output file and `<out>.progress` exist, the tool resumes automatically (skips already attempted positions and appends).

Example (streaming from stdin, JSONL output, split every 1M lines):
```bash
zcat runs/pass2_input.sfens.gz \
  | cargo run --release -p tools --bin generate_nnue_training_data -- \
      - runs/pass2.jsonl \
      --engine enhanced-nnue --output-format jsonl \
      --hash-mb 512 --multipv 2 --min-depth 3 \
      --split 1000000 --compress zst
```

Notes:
- Memory usage is bounded by the batch size, engine TT size, and output buffers â€” it does not grow with input size.
- When reading from `-` (stdin), input hash/size are omitted in the manifest (verification remains available for file inputs).

#### Teaching Quality Analyzer (Expected MultiPV Auto)

`analyze_teaching_quality` supports automatic MultiPV expectation resolution. The CLI accepts `--expected-multipv auto|<N>` (default: `auto`).

Resolution order when `auto`:
- Prefer final manifest field `aggregated.multipv` associated with the input
- Fallback to the (per-file) `multipv` in the nearest manifest
- If no manifest is present or fields are missing, fallback to `2`
- If a numeric value is specified at CLI, it always overrides the manifest

Example:
```bash
cargo run --release -p tools --bin analyze_teaching_quality -- \
  runs/final.jsonl --summary --manifest-autoload-mode strict
# summary line includes: "expected_mpv=<resolved>"
```

## Performance Considerations

- Use `--release` flag for production builds
- Opening book uses memory-mapped files for efficiency
- Position hashing optimized for fast lookups
- Move encoding reduces memory footprint

## License

MIT
